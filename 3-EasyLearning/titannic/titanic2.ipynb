{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 从CSV文件中读入数据\n",
    "data = pd.read_csv('../../../../dataset/titanic/train.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 取部分特征字段用于分类，清洗Age等数据\n",
    "data['Sex'] = data['Sex'].apply(lambda s: 1 if s == 'male' else 0)\n",
    "\n",
    "mean_age = data[\"Age\"].mean()\n",
    "data.loc[data.Age.isnull(), \"Age\"] = mean_age\n",
    "\n",
    "def get_title(name):\n",
    "    if(pd.isnull(name)):\n",
    "        return 'Null'\n",
    "    title_search = re.search('([A-Za-z]+)\\.', name)\n",
    "    if title_search:\n",
    "        return title_search.group(1).lower()\n",
    "    else:\n",
    "        return 'None'\n",
    "    \n",
    "titles = {'mr': 1,\n",
    "          'mrs': 2, 'mme': 2,\n",
    "          'ms': 3, 'miss': 3, 'mlle': 3,\n",
    "          'don': 4, 'sir': 4, 'jonkheer': 4,\n",
    "          'major': 4, 'col': 4, 'dr': 4, 'master': 4, 'capt': 4,\n",
    "          'dona': 5, 'lady': 5, 'countness': 5,\n",
    "          'rev': 6}\n",
    "data['Title'] = data['Name'].apply(lambda name: titles.get(get_title(name)))\n",
    "data['Honor'] = data['Title'].apply(lambda title: 1 if title == 4 or title == 5 else 0)\n",
    "\n",
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null int64\n",
      "Age            891 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          891 non-null object\n",
      "Embarked       891 non-null object\n",
      "Title          891 non-null float64\n",
      "Honor          891 non-null int64\n",
      "Deceased       891 non-null int64\n",
      "dtypes: float64(3), int64(8), object(4)\n",
      "memory usage: 104.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# 两种分类分别是幸存和死亡，‘Survived’字段是其中一种分类的标签，\n",
    "# 新增‘Deceased’字段表示第二段分类的标签，取值为‘Survived’字段取非\n",
    "data['Deceased'] = data['Survived'].apply(lambda s: int(not s))\n",
    "dataset_Y = data[['Deceased', 'Survived']]\n",
    "dataset_Y = dataset_Y.as_matrix()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.使用TFRecord格式存储数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 将train.csv转化为train.tfrecords\n",
    "def transform_to_tfrecord(data, tfrecord_file):    \n",
    "    def int_feature(value):\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "    \n",
    "    def float_feature(value):\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "    \n",
    "    writer = tf.python_io.TFRecordWriter(tfrecord_file)\n",
    "    for i in range(len(data)):\n",
    "        features = tf.train.Features(feature={\n",
    "                'Age': float_feature(data['Age'][i]),\n",
    "                'Survived': int_feature(data['Survived'][i]),\n",
    "                'Pclass': int_feature(data['Pclass'][i]),\n",
    "                'Parch': int_feature(data['Parch'][i]),\n",
    "                'SibSp': int_feature(data['SibSp'][i]),\n",
    "                'Sex': int_feature(data['Sex'][i]),\n",
    "                'Fare': float_feature(data['Fare'][i])})\n",
    "        example = tf.train.Example(features=features)\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "    \n",
    "transform_to_tfrecord(data, '../../../../dataset/titanic/train.tfrecords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.多线程方式读取TFRecord格式数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: [0 1 1 0 1 1 0 0 0 0]\n",
      "step 100: [0 0 0 0 0 0 0 0 0 0]\n",
      "step 200: [0 1 0 1 0 0 0 0 0 0]\n",
      "step 300: [1 0 1 1 0 0 1 0 1 0]\n",
      "step 400: [1 1 1 1 1 1 0 1 0 0]\n",
      "step 500: [1 1 1 1 0 0 1 1 0 1]\n",
      "step 600: [0 1 0 0 0 1 1 0 1 1]\n",
      "step 700: [0 1 0 0 0 0 0 1 0 0]\n",
      "step 800: [1 0 0 0 1 0 0 1 0 0]\n",
      "step 900: [0 0 0 1 0 0 0 0 1 0]\n",
      "step 1000: [1 1 0 0 1 1 0 0 0 1]\n",
      "step 1100: [0 1 0 0 1 1 0 0 1 1]\n",
      "step 1200: [0 0 0 0 0 0 1 0 1 0]\n",
      "step 1300: [0 0 0 1 0 0 1 0 1 1]\n",
      "step 1400: [1 0 0 1 0 0 1 0 0 0]\n",
      "step 1500: [1 0 1 0 1 0 0 0 0 0]\n",
      "step 1600: [1 0 1 1 0 0 1 1 1 0]\n",
      "step 1700: [1 1 0 0 0 0 1 1 0 0]\n",
      "step 1800: [0 1 1 0 0 1 0 1 1 0]\n",
      "step 1900: [0 0 1 0 1 1 1 1 1 0]\n",
      "step 2000: [0 0 1 0 0 0 0 0 1 1]\n",
      "step 2100: [1 0 0 0 0 0 1 0 1 0]\n",
      "step 2200: [0 0 1 0 0 0 1 0 0 0]\n",
      "step 2300: [0 1 0 1 0 0 0 0 1 0]\n",
      "step 2400: [0 0 0 1 0 1 0 0 0 1]\n",
      "step 2500: [0 1 1 0 0 0 1 1 0 0]\n",
      "step 2600: [0 0 0 0 0 0 0 0 0 1]\n",
      "step 2700: [0 1 0 1 1 0 1 0 1 0]\n",
      "step 2800: [1 1 1 1 0 1 0 0 0 1]\n",
      "step 2900: [1 0 0 0 0 0 0 0 0 0]\n",
      "step 3000: [0 1 1 0 0 1 0 0 1 0]\n",
      "step 3100: [0 0 0 0 0 0 0 1 0 1]\n",
      "step 3200: [0 0 0 0 1 0 1 1 1 1]\n",
      "step 3300: [0 0 1 1 1 1 1 0 0 0]\n",
      "step 3400: [0 0 0 1 0 0 0 1 0 0]\n",
      "step 3500: [1 1 1 1 1 0 0 0 1 1]\n",
      "step 3600: [0 0 1 0 0 1 0 0 0 0]\n",
      "step 3700: [1 0 1 0 0 0 0 1 0 1]\n",
      "step 3800: [0 0 0 0 1 1 1 0 0 0]\n",
      "step 3900: [1 0 0 0 0 0 1 0 0 0]\n",
      "step 4000: [0 1 0 1 0 0 0 0 1 0]\n",
      "step 4100: [0 1 1 1 0 0 0 0 1 0]\n",
      "step 4200: [1 0 0 0 1 0 0 1 0 1]\n",
      "step 4300: [1 0 0 0 0 0 1 0 0 0]\n",
      "step 4400: [1 1 0 0 1 0 0 0 1 0]\n",
      "step 4500: [0 1 1 1 0 1 1 1 0 0]\n",
      "step 4600: [1 1 1 0 0 1 1 0 0 0]\n",
      "step 4700: [0 0 0 1 0 1 0 0 0 1]\n",
      "step 4800: [0 1 0 0 0 1 0 0 0 0]\n",
      "step 4900: [1 0 1 0 0 1 0 1 0 1]\n",
      "step 5000: [0 0 0 0 0 0 0 1 0 0]\n",
      "step 5100: [1 0 0 1 1 0 0 0 1 0]\n",
      "step 5200: [0 1 0 0 1 1 0 1 1 0]\n",
      "step 5300: [0 0 0 0 1 0 1 1 1 1]\n",
      "step 5400: [0 0 0 1 1 1 0 1 1 1]\n",
      "step 5500: [0 1 1 0 1 0 0 0 0 0]\n",
      "step 5600: [1 0 0 0 0 0 0 0 0 0]\n",
      "step 5700: [0 0 1 0 0 1 0 0 0 0]\n",
      "step 5800: [0 1 0 1 0 1 0 1 1 0]\n",
      "step 5900: [1 0 0 0 0 0 0 1 1 0]\n",
      "step 6000: [1 0 0 0 0 1 0 1 0 1]\n",
      "step 6100: [0 0 1 0 0 0 0 0 1 1]\n",
      "step 6200: [0 0 0 0 1 0 1 1 1 1]\n",
      "step 6300: [0 1 0 0 1 0 1 0 1 1]\n",
      "step 6400: [1 1 1 0 1 1 0 0 0 0]\n",
      "step 6500: [0 0 0 0 1 0 0 1 1 1]\n",
      "step 6600: [0 1 0 0 0 1 1 1 0 0]\n",
      "step 6700: [0 0 0 1 0 1 1 0 0 0]\n",
      "step 6800: [1 1 0 0 0 0 0 0 1 1]\n",
      "step 6900: [0 0 1 1 1 0 0 0 1 0]\n",
      "step 7000: [1 1 0 0 1 0 1 0 1 0]\n",
      "step 7100: [0 1 1 1 0 0 1 0 0 0]\n",
      "step 7200: [0 1 0 1 1 1 0 0 0 1]\n",
      "step 7300: [1 0 1 1 0 0 0 0 0 0]\n",
      "step 7400: [1 1 0 1 0 0 1 0 0 1]\n",
      "step 7500: [0 0 0 0 1 1 0 0 1 0]\n",
      "step 7600: [1 1 1 1 0 0 1 1 0 0]\n",
      "step 7700: [0 0 1 0 1 1 0 1 1 0]\n",
      "step 7800: [1 0 0 0 0 0 1 0 0 0]\n",
      "step 7900: [0 0 0 0 0 0 0 0 1 0]\n",
      "step 8000: [1 1 0 0 1 0 1 1 0 1]\n",
      "step 8100: [0 0 0 1 1 0 0 0 0 1]\n",
      "step 8200: [1 0 1 1 0 0 1 0 1 1]\n",
      "step 8300: [0 1 1 0 0 1 0 0 0 0]\n",
      "step 8400: [1 0 0 0 1 0 0 0 1 0]\n",
      "step 8500: [0 0 0 0 0 1 1 0 0 0]\n",
      "step 8600: [0 0 0 0 1 0 0 0 0 1]\n",
      "step 8700: [0 0 1 1 1 1 0 1 0 0]\n",
      "step 8800: [0 1 1 1 0 1 0 1 0 0]\n",
      "step 8900: [0 0 0 0 1 1 0 0 1 0]\n",
      "Done training -- epoch limit reached\n"
     ]
    }
   ],
   "source": [
    "def read_and_decode(train_files, num_threads=2, num_epochs=100,\n",
    "                    batch_size=10, min_after_dequeue=10):\n",
    "    # read data from trainFile with TFRecord format\n",
    "    reader = tf.TFRecordReader()\n",
    "    filename_queue = tf.train.string_input_producer(train_files, num_epochs=num_epochs)\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    featuresdict = tf.parse_single_example(serialized_example, features={\n",
    "            'Age': tf.FixedLenFeature([], tf.float32),\n",
    "            'Survived': tf.FixedLenFeature([], tf.int64),\n",
    "            'Pclass': tf.FixedLenFeature([], tf.int64),\n",
    "            'Parch': tf.FixedLenFeature([], tf.int64),\n",
    "            'SibSp': tf.FixedLenFeature([], tf.int64),\n",
    "            'Sex': tf.FixedLenFeature([], tf.int64),\n",
    "            'Fare': tf.FixedLenFeature([], tf.float32)})\n",
    "    \n",
    "    # decode features to same format of float32\n",
    "    labels = featuresdict.pop('Survived')\n",
    "    features = [tf.cast(value, tf.float32) for value in featuresdict.values()]\n",
    "    \n",
    "    # get data with shuffle hatch and return\n",
    "    features, labels = tf.train.shuffle_batch(\n",
    "        [features, labels],\n",
    "        batch_size=batch_size,\n",
    "        num_threads=num_threads,\n",
    "        capacity=min_after_dequeue + 3 * batch_size,\n",
    "        min_after_dequeue=min_after_dequeue)\n",
    "    return features, labels\n",
    "\n",
    "def train_with_queuerunner():\n",
    "    x, y = read_and_decode(['../../../../dataset/titanic/train.tfrecords'])\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        tf.group(tf.global_variables_initializer(),\n",
    "                tf.local_variables_initializer()).run()\n",
    "        \n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        \n",
    "        try:\n",
    "            step = 0\n",
    "            while not coord.should_stop():\n",
    "                # Run training steps or whatever\n",
    "                features, labels = sess.run([x, y])\n",
    "                if(step%100 == 0):\n",
    "                    print('step %d:' % step, labels)\n",
    "                step += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done training -- epoch limit reached')\n",
    "        finally:\n",
    "            # when done, ask the thread to stop.\n",
    "            coord.request_stop()\n",
    "            \n",
    "        # wait for threads to finish.\n",
    "        coord.join(threads)\n",
    "        \n",
    "train_with_queuerunner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000, total loss = 4713.340104389\n",
      "Epoch: 0010, total loss = 3237.428724157\n",
      "Epoch: 0020, total loss = 845.970993774\n",
      "Epoch: 0030, total loss = 661.420603499\n",
      "Epoch: 0040, total loss = 585.708687306\n",
      "Epoch: 0050, total loss = 539.824730469\n",
      "Epoch: 0060, total loss = 506.740099301\n",
      "Epoch: 0070, total loss = 483.490554612\n",
      "Epoch: 0080, total loss = 467.280953822\n",
      "Epoch: 0090, total loss = 455.878746660\n",
      "Training complete!\n",
      "Accuracy on validation set: 0.670391083\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 声明输入数据占位符\n",
    "# shape参数的第一个元素为None，表示可以同时放入任意条记录\n",
    "with tf.name_scope('input'):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 7], name='input_x')\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 2], name='input_y')\n",
    "\n",
    "with tf.name_scope('classifier'):\n",
    "    # 声明变量\n",
    "    W = tf.Variable(tf.random_normal([7, 2]), name='weights')\n",
    "    b = tf.Variable(tf.zeros([2]), name='bias')\n",
    "    y_pred = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "    \n",
    "    # 添加直方图参数概要记录算子\n",
    "    tf.summary.histogram('weights', W)\n",
    "    tf.summary.histogram('bias',b)\n",
    "\n",
    "with tf.name_scope('cost'):\n",
    "    # 使用交叉熵作为代价函数\n",
    "    cross_entropy = - tf.reduce_sum(y * tf.log(y_pred + 1e-10))\n",
    "    # 批量样本的代价值为所有样本交叉熵的平均值\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "    # 添加损失函数标量概要\n",
    "    tf.summary.scalar('loss', cost)\n",
    "\n",
    "# 使用随机梯度下降算法优化器来最小化代价，系统自动构建反向传播部分的计算图\n",
    "train_op = tf.train.GradientDescentOptimizer(0.0002).minimize(cost)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_pred = tf.equal(tf.argmax(y, 1), tf.argmax(y_pred, 1))\n",
    "    acc_op = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    tf.summary.scalar('accuracy', acc_op)\n",
    "\n",
    "# 保存模型\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 创建概要写入操作\n",
    "    # Tensorboard可通过命令‘tensorboard --logdir=./logs’来启动\n",
    "    writer = tf.summary.FileWriter('./logs', sess.graph)\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # 初始化所有变量，必须最先执行\n",
    "    tf.global_variables_initializer().run()\n",
    "    # 以下为训练迭代，迭代10轮\n",
    "    for epoch in range(100):\n",
    "        total_loss = 0.\n",
    "        for i in range(len(X_train)):\n",
    "            # 通过session.run接口触发运行\n",
    "            _, loss = sess.run([train_op, cost], feed_dict = {X: [X_train[i]], y: [y_train[i]]})\n",
    "            total_loss += loss\n",
    "        \n",
    "        summary, accuracy = sess.run([merged, acc_op], feed_dict = {X: X_train, y: y_train})\n",
    "        writer.add_summary(summary, epoch)\n",
    "        \n",
    "        if(epoch%10 == 0):\n",
    "            print('Epoch: %04d, total loss = %.9f' % (epoch, total_loss))\n",
    "            saver.save(sess,\"./mymodel.ckpt\", global_step=epoch)\n",
    "    \n",
    "    writer.close()\n",
    "    saver.save(sess,\"./mymodel.ckpt\")\n",
    "    print('Training complete!')\n",
    "    \n",
    "    # 评估校验数据集上的准确率\n",
    "    pred = sess.run(y_pred, feed_dict={X: X_test})\n",
    "    correct = np.equal(np.argmax(pred, 1), np.argmax(y_test, 1))\n",
    "    accuracy = np.mean(correct.astype(np.float32))\n",
    "    print(\"Accuracy on validation set: %.9f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./mymodel.ckpt\n"
     ]
    }
   ],
   "source": [
    "# 读入测试数据集并完成预处理\n",
    "testdata = pd.read_csv('../../../../dataset/titanic/test.csv')\n",
    "\n",
    "mean_age = testdata[\"Age\"].mean()\n",
    "testdata.loc[testdata.Age.isnull(), \"Age\"] = mean_age\n",
    "\n",
    "testdata = testdata.fillna(0)\n",
    "testdata['Sex'] = testdata['Sex'].apply(lambda s:1 if s == 'male' else 0)\n",
    "\n",
    "testdata['Title'] = testdata['Name'].apply(lambda name: titles.get(get_title(name)))\n",
    "testdata['Honor'] = testdata['Title'].apply(lambda title: 1 if title == 4 or title == 5 else 0)\n",
    "\n",
    "X_test = testdata[['Sex', 'Age', 'Pclass', 'SibSp', 'Parch', 'Fare', 'Honor']]\n",
    "\n",
    "# 开启session进行预测\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './mymodel.ckpt')\n",
    "    # 正向传播计算\n",
    "    predictions = np.argmax(sess.run(y_pred, feed_dict={X: X_test}), 1)\n",
    "    # 构建提交结果的数据结构，并将结果存为csv文件\n",
    "    submission = pd.DataFrame({\"PassengerId\": testdata[\"PassengerId\"], \"Survived\": predictions})\n",
    "    submission.to_csv(\"../../../../dataset/titanic/titanic_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
