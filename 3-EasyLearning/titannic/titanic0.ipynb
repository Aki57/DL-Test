{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 从CSV文件中读入数据\n",
    "data = pd.read_csv('../../../../dataset/titanic/train.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 取部分特征字段用于分类，并将所有缺失的字段填充为0\n",
    "data['Sex'] = data['Sex'].apply(lambda s: 1 if s == 'male' else 0)\n",
    "data = data.fillna(0)\n",
    "dataset_X = data[['Sex', 'Age', 'Pclass', 'SibSp', 'Parch', 'Fare']]\n",
    "dataset_X = dataset_X.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 13 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null int64\n",
      "Age            891 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          891 non-null object\n",
      "Embarked       891 non-null object\n",
      "Deceased       891 non-null int64\n",
      "dtypes: float64(2), int64(7), object(4)\n",
      "memory usage: 90.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# 两种分类分别是幸存和死亡，‘Survived’字段是其中一种分类的标签，\n",
    "# 新增‘Deceased’字段表示第二段分类的标签，取值为‘Survived’字段取非\n",
    "data['Deceased'] = data['Survived'].apply(lambda s: int(not s))\n",
    "dataset_Y = data[['Deceased', 'Survived']]\n",
    "dataset_Y = dataset_Y.as_matrix()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.初步处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 使用sklearn的train_test_split函数将标记数据切分为“训练数据集和验证集”\n",
    "# 将全部标记数据随机洗牌后切分，其中验证数据占20%，由test_size参数指定\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dataset_X, dataset_Y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000, total loss = 2382.947940569\n",
      "Epoch: 0010, total loss = 1116.872451963\n",
      "Epoch: 0020, total loss = 1071.427973176\n",
      "Epoch: 0030, total loss = 1054.925065220\n",
      "Epoch: 0040, total loss = 1011.089648127\n",
      "Epoch: 0050, total loss = 1004.657435875\n",
      "Epoch: 0060, total loss = 960.662514119\n",
      "Epoch: 0070, total loss = 961.212779934\n",
      "Epoch: 0080, total loss = 957.269151649\n",
      "Epoch: 0090, total loss = 954.692457028\n",
      "Training complete!\n",
      "Accuracy on validation set: 0.636871517\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 声明输入数据占位符\n",
    "# shape参数的第一个元素为None，表示可以同时放入任意条记录\n",
    "X = tf.placeholder(tf.float32, shape=[None, 6])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "# 声明变量\n",
    "W = tf.Variable(tf.random_normal([6, 2]), name='weights')\n",
    "b = tf.Variable(tf.zeros([2]), name='bias')\n",
    "\n",
    "y_pred = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# 使用交叉熵作为代价函数\n",
    "cross_entropy = - tf.reduce_sum(y * tf.log(y_pred + 1e-10))\n",
    "# 批量样本的代价值为所有样本交叉熵的平均值\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# 使用随机梯度下降算法优化器来最小化代价，系统自动构建反向传播部分的计算图\n",
    "train_op = tf.train.GradientDescentOptimizer(0.001).minimize(cost)\n",
    "\n",
    "# 保存模型\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 初始化所有变量，必须最先执行\n",
    "    tf.global_variables_initializer().run()\n",
    "    # 以下为训练迭代，迭代10轮\n",
    "    for epoch in range(100):\n",
    "        total_loss = 0.\n",
    "        for i in range(len(X_train)):\n",
    "            feed = {X: [X_train[i]], y: [y_train[i]]}\n",
    "            # 通过session.run接口触发运行\n",
    "            _, loss = sess.run([train_op, cost], feed_dict = feed)\n",
    "            total_loss += loss\n",
    "        if(epoch%10 == 0):\n",
    "            print('Epoch: %04d, total loss = %.9f' % (epoch, total_loss))\n",
    "            saver.save(sess,\"./mymodel.ckpt\", global_step=epoch)\n",
    "    print('Training complete!')\n",
    "    \n",
    "    # 评估校验数据集上的准确率\n",
    "    pred = sess.run(y_pred, feed_dict={X: X_test})\n",
    "    correct = np.equal(np.argmax(pred, 1), np.argmax(y_test, 1))\n",
    "    accuracy = np.mean(correct.astype(np.float32))\n",
    "    print(\"Accuracy on validation set: %.9f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./mymodel.ckpt\n"
     ]
    }
   ],
   "source": [
    "# 读入测试数据集并完成预处理\n",
    "testdata = pd.read_csv('../../../../dataset/titanic/test.csv')\n",
    "testdata = testdata.fillna(0)\n",
    "testdata['Sex'] = testdata['Sex'].apply(lambda s:1 if s == 'male' else 0)\n",
    "X_test = testdata[['Sex', 'Age', 'Pclass', 'SibSp', 'Parch', 'Fare']]\n",
    "\n",
    "# 开启session进行预测\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './mymodel.ckpt')\n",
    "    # 正向传播计算\n",
    "    predictions = np.argmax(sess.run(y_pred, feed_dict={X: X_test}), 1)\n",
    "    # 构建提交结果的数据结构，并将结果存为csv文件\n",
    "    submission = pd.DataFrame({\"PassengerId\": testdata[\"PassengerId\"], \"Survived\": predictions})\n",
    "    submission.to_csv(\"titanic_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
